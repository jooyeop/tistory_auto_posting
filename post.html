<h3 data-ke-size='size23'><b><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>Fine-Tuning Pre-Trained Language Models Effectively by Optimizing Subnetworks Adaptively</span></b><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'></span></h3><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>
 Large-scale pre-trained language models have achieved impressive results on a
wide range of downstream tasks recently. However, fine-tuning an extremely
large-scale pre-trained language model on limited target datasets is often
plagued by overfitting and representation degradation. In this paper, we
propose a Dynamic Parameter Selection (DPS) algorithm for the large-scale
pre-trained models during fine-tuning, which adaptively selects a more
promising subnetwork to perform staging updates based on gradients of
back-propagation. Experiments on the GLUE benchmark show that DPS outperforms
previous fine-tuning methods in terms of overall performance and stability, and
consistently achieves better results with variable pre-trained language models.
In addition, DPS brings a large magnitude of improvement in out-of-domain
transferring experiments and low-resource scenarios, which shows that it can
maintain stable general contextual features and reduce the representation
collapse. We release our code at this https URL
</span></p><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>대규모 사전 교육 언어 모델은 다음과 같은 인상적인 결과를 얻었다.
최근 광범위한 다운스트림 작업. 그러나 극도로 미세하게 조정한다.
제한된 대상 데이터 세트에 대한 대규모 사전 훈련된 언어 모델은 종종
과적합 및 표현력 저하에 시달린다. 이 논문에서, 우리는
대규모 동적 파라미터 선택(DPS) 알고리즘을 제안하다
미세 조정 중에 사전 훈련된 모델, 적응적으로 더 많은 것을 선택한다.
경사도를 기반으로 스테이징 업데이트를 수행할 수 있는 유망한 하위 네트워크
역행의 GLUE 벤치마크에 대한 실험 결과 DPS가 우수한 것으로 나타났습니다.
전체적인 성능 및 안정성 측면에서 이전의 미세 조정 방법들,
가변적인 사전 훈련된 언어 모델을 사용하여 지속적으로 더 나은 결과를 달성한다.
또한 DPS는 도메인 외부에서 큰 폭으로 개선됩니다.
전송 실험 및 저자원 시나리오, 이 시나리오가 다음을 수행할 수 있음을 보여줍니다.
안정된 일반적인 문맥적 특징을 유지하고 표현을 줄인다.
무너지다 이 https URL에서 코드를 공개합니다.&nbsp;</span></p><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'>&nbsp;</p><h3 data-ke-size='size23'><b><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>Query-based Instance Discrimination Network for Relational Triple Extraction</span></b><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'></span></h3><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>
 Joint entity and relation extraction has been a core task in the field of
information extraction. Recent approaches usually consider the extraction of
relational triples from a stereoscopic perspective, either learning a
relation-specific tagger or separate classifiers for each relation type.
However, they still suffer from error propagation, relation redundancy and lack
of high-level connections between triples. To address these issues, we propose
a novel query-based approach to construct instance-level representations for
relational triples. By metric-based comparison between query embeddings and
token embeddings, we can extract all types of triples in one step, thus
eliminating the error propagation problem. In addition, we learn the
instance-level representation of relational triples via contrastive learning.
In this way, relational triples can not only enclose rich class-level semantics
but also access to high-order global connections. Experimental results show
that our proposed method achieves the state of the art on five widely used
benchmarks.

    </span></p><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>공동 실체와 관계 추출은 다음 분야의 핵심 과제였다.
정보 추출 최근 접근 방식은 일반적으로 추출을 고려한다.
입체적인 관점에서 상대적인 3배, 또는 학습.
관계별 태그 또는 각 관계 유형에 대한 별도의 분류자.
그러나 여전히 오류 전파, 관계 중복 및 부족에 시달린다.
세 개 사이의 고차원적인 연결의. 이러한 문제를 해결하기 위해, 우리는 제안합니다.
인스턴스 수준 표현을 구성하기 위한 새로운 쿼리 기반 접근법
관계형 3중주 쿼리 임베딩 간의 메트릭 기반 비교
토큰 임베딩, 우리는 한 단계에서 모든 유형의 트리플을 추출할 수 있다.
오류 전파 문제를 제거합니다. 게다가, 우리는 배운다.
대조 학습을 통한 관계형 트리플의 인스턴스 수준 표현.
이러한 방식으로, 관계형 삼중항은 풍부한 클래스 수준의 의미론만을 포함할 수 없다.
고급 글로벌 연결에도 액세스할 수 있습니다. 실험 결과는 다음과 같다.
우리가 제안한 방법이 널리 사용되는 다섯 가지에 대한 최첨단 기술을 달성한다는 것.
벤치마크&nbsp;</span></p><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'>&nbsp;</p><h3 data-ke-size='size23'><b><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>Latent Prompt Tuning for Text Summarization</span></b><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'></span></h3><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>
 Prompts with different control signals (e.g., length, keywords, etc.) can be
used to control text summarization. When control signals are available, they
can control the properties of generated summaries and potentially improve
summarization quality (since more information are given). Unfortunately,
control signals are not already available during inference time. In this paper,
we propose Lotus (shorthand for Latent Prompt Tuning for Summarization), which
is a single model that can be applied in both controlled and uncontrolled
(without control signals) modes. During training, Lotus learns latent prompt
representations from prompts with gold control signals using a contrastive
learning objective. Experiments show Lotus in uncontrolled mode consistently
improves upon strong (uncontrollable) summarization models across four
different summarization datasets. We also demonstrate generated summaries can
be controlled using prompts with user specified control tokens.

    </span></p><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>제어 신호(예: 길이, 키워드 등)가 다른 프롬프트는 다음과 같을 수 있다.
텍스트 요약을 제어하는 데 사용됩니다. 제어 신호를 사용할 수 있을 때,
생성된 요약의 속성을 제어하고 잠재적으로 개선할 수 있습니다.
요약 품질(추가 정보가 제공되므로). 불행하게도,
추론 시간 중에는 제어 신호를 이미 사용할 수 없습니다. 이 논문에서,
우리는 Lotus(요약화를 위한 잠재 프롬프트 튜닝의 줄임말)를 제안합니다.
제어 및 비제어 모두에 적용할 수 있는 단일 모델입니다.
(제어 신호 없음) 모드. 교육 중에 Lotus는 잠재된 프롬프트를 학습합니다.
대비를 사용하여 금색 제어 신호를 사용한 프롬프트의 표현
학습 목표 실험에 따르면 Lotus는 제어되지 않는 모드입니다.
4가지에 걸쳐 강력한(제어할 수 없는) 요약 모델을 개선합니다.
서로 다른 요약 데이터 세트. 또한 생성된 요약 내용을 시연합니다.
사용자 지정 제어 토큰이 있는 프롬프트를 사용하여 제어됩니다.&nbsp;</span></p><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'>&nbsp;</p>