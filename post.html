<h3 data-ke-size='size23'><b><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>A Weakly-Supervised Streaming Multilingual Speech Model with Truly Zero-Shot Capability</span></b><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'></span></h3><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>
 In this paper, we introduce our work of building a Streaming Multilingual
Speech Model (SM2), which can transcribe or translate multiple spoken languages
into texts of the target language. The backbone of SM2 is Transformer
Transducer, which has high streaming capability. Instead of human labeled
speech translation (ST) data, SM2 models are trained using weakly supervised
data generated by converting the transcriptions in speech recognition corpora
with a machine translation service. With 351 thousand hours of anonymized
speech training data from 25 languages, SM2 models achieve comparable or even
better ST quality than some recent popular large-scale non-streaming speech
models. More importantly, we show that SM2 has the truly zero-shot capability
when expanding to new target languages, yielding high quality ST results for
{source-speech, target-text} pairs that are not seen during training.

    </span></p><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>본 논문에서는 스트리밍 다국어 구축 작업을 소개합니다.
여러 음성 언어를 기록하거나 번역할 수 있는 음성 모델(SM2)
대상 언어의 텍스트로 변환합니다. SM2의 백본은 트랜스포머이다.
스트리밍 능력이 높은 변환기. 라벨이 붙은 사람 대신
음성 번역(ST) 데이터, SM2 모델은 약하게 감독된 것을 사용하여 훈련된다.
음성 인식 말뭉치의 전사를 변환하여 생성된 데이터
기계 번역 서비스로요. 351,000시간 동안 익명으로
25개 언어의 음성 훈련 데이터, SM2 모델은 동등하거나 심지어 달성한다.
최근 인기 있는 대규모 비스트리밍 연설보다 더 나은 ST 품질
더 중요한 것은 SM2가 진정한 제로샷 기능을 가지고 있다는 것이다.
새로운 대상 언어로 확장할 때 고품질 ST 결과를 산출합니다.
교육 중에 볼 수 없는 {source-message, target-text} 쌍.&nbsp;</span></p><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'>&nbsp;</p><h3 data-ke-size='size23'><b><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>Time-aware Prompting for Text Generation</span></b><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'></span></h3><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>
 In this paper, we study the effects of incorporating timestamps, such as
document creation dates, into generation systems. Two types of time-aware
prompts are investigated: (1) textual prompts that encode document timestamps
in natural language sentences; and (2) linear prompts that convert timestamps
into continuous vectors. To explore extrapolation to future data points, we
further introduce a new data-to-text generation dataset, TempWikiBio,
containing more than 4 millions of chronologically ordered revisions of
biographical articles from English Wikipedia, each paired with structured
personal profiles. Through data-to-text generation on TempWikiBio, text-to-text
generation on the content transfer dataset, and summarization on XSum, we show
that linear prompts on encoder and textual prompts improve the generation
quality on all datasets. Despite having less performance drop when testing on
data drawn from a later time, linear prompts focus more on non-temporal
information and are less sensitive to the given timestamps, according to human
evaluations and sensitivity analyses. Meanwhile, textual prompts establish the
association between the given timestamps and the output dates, yielding more
factual temporal information in the output.

    </span></p><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>&nbsp;</span></p><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'>&nbsp;</p><h3 data-ke-size='size23'><b><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic</span></b><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'></span></h3><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>
 Through their transfer learning abilities, highly-parameterized large
pre-trained language models have dominated the NLP landscape for a multitude of
downstream language tasks. Though linguistically proficient, the inability of
these models to incorporate the learning of non-linguistic entities (numerals
and arithmetic reasoning) limits their usage for tasks that require numeric
comprehension or strict mathematical reasoning. However, as we illustrate in
this paper, building a general purpose language model that also happens to be
proficient in mathematical reasoning is not as straight-forward as training it
on a numeric dataset. In this work, we develop a novel framework that enables
language models to be mathematically proficient while retaining their
linguistic prowess. Specifically, we offer information-theoretic interventions
to overcome the catastrophic forgetting of linguistic skills that occurs while
injecting non-linguistic skills into language models.

    </span></p><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'><span style='font-family: 'Noto Sans Demilight', 'Noto Sans KR';'>&nbsp;</span></p><p data-ke-size='size18'>&nbsp;</p><p data-ke-size='size18'>&nbsp;</p>